{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fruit and Vegetable Classification\n\n\nHaving 3861 images of 36 different fruits/vegetables\n\n![fruit vegetable](https://cdn.pixabay.com/photo/2018/04/09/18/23/fruit-3304977_1280.jpg)","metadata":{}},{"cell_type":"markdown","source":"<h1>Table of contents</h1>\n \n\n<ul>\n<li><a href=\"#1\"><strong>1. Loading and preprocessing</strong></a>\n</ul>\n    \n<ul>\n<li><a href=\"#2\"><strong>2. Load the Images with a generator and Data Augmentation</strong></a>\n</ul>\n\n<ul>\n<li><a href=\"#3\"><strong>3. Train the model</strong></a>\n</ul>\n\n<ul>\n<li><a href=\"#4\"><strong>4. Visualize the result</strong></a>\n</ul>\n\n<ul>\n<li><a href=\"#5\"><strong>5. Class activation heatmap for image classification</strong></a>\n</ul>\n\n<ul>\n<li><a href=\"#6\"><strong>6. Deploy the model</strong></a>\n</ul>\n\n# Context\n\nThis dataset contains images of the following food items:\n\n- **fruits**: banana, apple, pear, grapes, orange, kiwi, watermelon, pomegranate, pineapple, mango\n- **vegetables**: cucumber, carrot, capsicum, onion, potato, lemon, tomato, raddish, beetroot, cabbage, lettuce, spinach, soy bean, cauliflower, bell pepper, chilli pepper, turnip, corn, sweetcorn, sweet potato, paprika, jalepe√±o, ginger, garlic, peas, eggplant\n\n\n# Content\nThis dataset contains three folders:\n\n- train (100 images each)\n- test (10 images each)\n- validation (10 images each)\neach of the above folders contains subfolders for different fruits and vegetables wherein the images for respective food items are present\n","metadata":{}},{"cell_type":"markdown","source":"# 1. Loading and preprocessing<a class=\"anchor\" id=\"1\"></a><a class=\"anchor\" id=\"1\"></a>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n# Create a list with the filepaths for training and testing\ntrain_dir = Path('../input/fruit-and-vegetable-image-recognition/train')\ntrain_filepaths = list(train_dir.glob(r'**/*.jpg'))\n\ntest_dir = Path('../input/fruit-and-vegetable-image-recognition/test')\ntest_filepaths = list(test_dir.glob(r'**/*.jpg'))\n\nval_dir = Path('../input/fruit-and-vegetable-image-recognition/validation')\nval_filepaths = list(test_dir.glob(r'**/*.jpg'))\n\ndef proc_img(filepath):\n    \"\"\" Create a DataFrame with the filepath and the labels of the pictures\n    \"\"\"\n\n    labels = [str(filepath[i]).split(\"/\")[-2] \\\n              for i in range(len(filepath))]\n\n    filepath = pd.Series(filepath, name='Filepath').astype(str)\n    labels = pd.Series(labels, name='Label')\n\n    # Concatenate filepaths and labels\n    df = pd.concat([filepath, labels], axis=1)\n\n    # Shuffle the DataFrame and reset index\n    df = df.sample(frac=1).reset_index(drop = True)\n    \n    return df\n\ntrain_df = proc_img(train_filepaths)\ntest_df = proc_img(test_filepaths)\nval_df = proc_img(val_filepaths)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T11:47:54.425580Z","iopub.execute_input":"2023-07-28T11:47:54.426012Z","iopub.status.idle":"2023-07-28T11:48:01.854110Z","shell.execute_reply.started":"2023-07-28T11:47:54.425898Z","shell.execute_reply":"2023-07-28T11:48:01.853120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('-- Training set --\\n')\nprint(f'Number of pictures: {train_df.shape[0]}\\n')\nprint(f'Number of different labels: {len(train_df.Label.unique())}\\n')\nprint(f'Labels: {train_df.Label.unique()}')","metadata":{"execution":{"iopub.status.busy":"2023-07-28T11:48:43.664020Z","iopub.execute_input":"2023-07-28T11:48:43.664383Z","iopub.status.idle":"2023-07-28T11:48:43.677121Z","shell.execute_reply.started":"2023-07-28T11:48:43.664350Z","shell.execute_reply":"2023-07-28T11:48:43.675779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The DataFrame with the filepaths in one column and the labels in the other one\ntrain_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T11:48:56.675691Z","iopub.execute_input":"2023-07-28T11:48:56.676068Z","iopub.status.idle":"2023-07-28T11:48:56.692916Z","shell.execute_reply.started":"2023-07-28T11:48:56.676033Z","shell.execute_reply":"2023-07-28T11:48:56.691742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataFrame with one Label of each category\ndf_unique = train_df.copy().drop_duplicates(subset=[\"Label\"]).reset_index()\n\n# Display some pictures of the dataset\nfig, axes = plt.subplots(nrows=6, ncols=6, figsize=(8, 7),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(df_unique.Filepath[i]))\n    ax.set_title(df_unique.Label[i], fontsize = 12)\nplt.tight_layout(pad=0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-28T11:49:16.895152Z","iopub.execute_input":"2023-07-28T11:49:16.895561Z","iopub.status.idle":"2023-07-28T11:49:25.234508Z","shell.execute_reply.started":"2023-07-28T11:49:16.895530Z","shell.execute_reply":"2023-07-28T11:49:25.233624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Load the Images with a generator and Data Augmentation<a class=\"anchor\" id=\"2\"></a>","metadata":{}},{"cell_type":"code","source":"train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n)\n\ntest_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n)\n\ntrain_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=0,\n    rotation_range=30,\n    zoom_range=0.15,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.15,\n    horizontal_flip=True,\n    fill_mode=\"nearest\"\n)\n\nval_images = train_generator.flow_from_dataframe(\n    dataframe=val_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=0,\n    rotation_range=30,\n    zoom_range=0.15,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.15,\n    horizontal_flip=True,\n    fill_mode=\"nearest\"\n)\n\ntest_images = test_generator.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=False\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T11:51:32.044974Z","iopub.execute_input":"2023-07-28T11:51:32.045346Z","iopub.status.idle":"2023-07-28T11:51:33.754502Z","shell.execute_reply.started":"2023-07-28T11:51:32.045314Z","shell.execute_reply":"2023-07-28T11:51:33.752957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the pretained model\npretrained_model = tf.keras.applications.MobileNetV2(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='avg'\n)\npretrained_model.trainable = False","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-28T11:51:52.473866Z","iopub.execute_input":"2023-07-28T11:51:52.474413Z","iopub.status.idle":"2023-07-28T11:51:55.901532Z","shell.execute_reply.started":"2023-07-28T11:51:52.474371Z","shell.execute_reply":"2023-07-28T11:51:55.900582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Train the model<a class=\"anchor\" id=\"3\"></a>","metadata":{}},{"cell_type":"code","source":"inputs = pretrained_model.input\n\nx = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\n\noutputs = tf.keras.layers.Dense(36, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    train_images,\n    validation_data=val_images,\n    batch_size = 32,\n    epochs=5,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=2,\n            restore_best_weights=True\n        )\n    ]\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-28T11:55:01.958856Z","iopub.status.idle":"2023-07-28T11:55:01.959426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot()\nplt.title(\"Accuracy\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-19T02:59:03.467022Z","iopub.execute_input":"2022-10-19T02:59:03.467313Z","iopub.status.idle":"2022-10-19T02:59:03.638854Z","shell.execute_reply.started":"2022-10-19T02:59:03.467279Z","shell.execute_reply":"2022-10-19T02:59:03.637808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history)[['loss','val_loss']].plot()\nplt.title(\"Loss\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-19T02:59:03.64059Z","iopub.execute_input":"2022-10-19T02:59:03.641099Z","iopub.status.idle":"2022-10-19T02:59:03.794027Z","shell.execute_reply.started":"2022-10-19T02:59:03.64106Z","shell.execute_reply":"2022-10-19T02:59:03.793082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Visualize the result<a class=\"anchor\" id=\"4\"></a>","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict the label of the test_images\npred = model.predict(test_images)\npred = np.argmax(pred,axis=1)\n\n# Map the label\nlabels = (train_images.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred = [labels[k] for k in pred]\n\ny_test = [labels[k] for k in test_images.classes]","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-10-19T03:05:32.79776Z","iopub.execute_input":"2022-10-19T03:05:32.798111Z","iopub.status.idle":"2022-10-19T03:05:51.242151Z","shell.execute_reply.started":"2022-10-19T03:05:32.79808Z","shell.execute_reply":"2022-10-19T03:05:51.241271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels.items()","metadata":{"execution":{"iopub.status.busy":"2022-10-19T03:05:54.058478Z","iopub.execute_input":"2022-10-19T03:05:54.058858Z","iopub.status.idle":"2022-10-19T03:05:54.064893Z","shell.execute_reply.started":"2022-10-19T03:05:54.058825Z","shell.execute_reply":"2022-10-19T03:05:54.063822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nacc = accuracy_score(y_test, pred)\nprint(f'Accuracy on the test set: {100*acc:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2022-10-19T03:06:01.751859Z","iopub.execute_input":"2022-10-19T03:06:01.752181Z","iopub.status.idle":"2022-10-19T03:06:02.269974Z","shell.execute_reply.started":"2022-10-19T03:06:01.752149Z","shell.execute_reply":"2022-10-19T03:06:02.269075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncf_matrix = confusion_matrix(y_test, pred, normalize='true')\nplt.figure(figsize = (15,10))\nsns.heatmap(cf_matrix, \n            annot=True, \n            xticklabels = sorted(set(y_test)), \n            yticklabels = sorted(set(y_test)),\n            )\nplt.title('Normalized Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-19T03:06:02.271598Z","iopub.execute_input":"2022-10-19T03:06:02.272163Z","iopub.status.idle":"2022-10-19T03:06:06.44453Z","shell.execute_reply.started":"2022-10-19T03:06:02.272125Z","shell.execute_reply":"2022-10-19T03:06:06.443637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display some pictures of the dataset with their labels and the predictions\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(test_df.Filepath.iloc[i]))\n    ax.set_title(f\"True: {test_df.Label.iloc[i]}\\nPredicted: {pred[i]}\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-19T03:06:06.446258Z","iopub.execute_input":"2022-10-19T03:06:06.446795Z","iopub.status.idle":"2022-10-19T03:06:10.953921Z","shell.execute_reply.started":"2022-10-19T03:06:06.446755Z","shell.execute_reply":"2022-10-19T03:06:10.94967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Class activation heatmap for image classification<a class=\"anchor\" id=\"5\"></a>\n### Grad-CAM class activation visualization\n*Code adapted from keras.io*","metadata":{}},{"cell_type":"code","source":"import matplotlib.cm as cm\n\ndef get_img_array(img_path, size):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n    array = tf.keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size \"size\"\n    array = np.expand_dims(array, axis=0)\n    return array\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\ndef save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n    # Load the original image\n    img = tf.keras.preprocessing.image.load_img(img_path)\n    img = tf.keras.preprocessing.image.img_to_array(img)\n\n    # Rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n\n    # Save the superimposed image\n    superimposed_img.save(cam_path)\n\n    # Display Grad CAM\n#     display(Image(cam_path))\n    \n    return cam_path\n    \npreprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\ndecode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n\nlast_conv_layer_name = \"Conv_1\"\nimg_size = (224,224)\n\n# Remove last layer's softmax\nmodel.layers[-1].ativation = None","metadata":{"execution":{"iopub.status.busy":"2022-10-19T03:06:10.955518Z","iopub.execute_input":"2022-10-19T03:06:10.956033Z","iopub.status.idle":"2022-10-19T03:06:10.973128Z","shell.execute_reply.started":"2022-10-19T03:06:10.955995Z","shell.execute_reply":"2022-10-19T03:06:10.972385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the part of the pictures used by the neural network to classify the pictures\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    img_path = test_df.Filepath.iloc[i]\n    img_array = preprocess_input(get_img_array(img_path, size=img_size))\n    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n    cam_path = save_and_display_gradcam(img_path, heatmap)\n    ax.imshow(plt.imread(cam_path))\n    ax.set_title(f\"True: {test_df.Label.iloc[i]}\\nPredicted: {pred[i]}\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-19T03:06:10.974719Z","iopub.execute_input":"2022-10-19T03:06:10.975451Z","iopub.status.idle":"2022-10-19T03:06:19.739626Z","shell.execute_reply.started":"2022-10-19T03:06:10.975406Z","shell.execute_reply":"2022-10-19T03:06:19.738596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def output(location):\n    from tensorflow.keras.preprocessing.image import load_img,img_to_array\n    img=load_img(location,target_size=(224,224,3))\n    img=img_to_array(img)\n    img=img/255\n    img=np.expand_dims(img,[0])\n    answer=model.predict(img)\n    y_class = answer.argmax(axis=-1)\n    y = \" \".join(str(x) for x in y_class)\n    y = int(y)\n    res = labels[y]\n    return res","metadata":{"execution":{"iopub.status.busy":"2022-10-19T03:06:19.741242Z","iopub.execute_input":"2022-10-19T03:06:19.741606Z","iopub.status.idle":"2022-10-19T03:06:19.75763Z","shell.execute_reply.started":"2022-10-19T03:06:19.741568Z","shell.execute_reply":"2022-10-19T03:06:19.756573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display\nfrom PIL import Image\n\nlocation = '../input/fruit-and-vegetable-image-recognition/test/cabbage/Image_1.jpg'\ndisplay(Image.open(location))\n","metadata":{"execution":{"iopub.status.busy":"2022-10-19T03:06:19.759Z","iopub.execute_input":"2022-10-19T03:06:19.759313Z","iopub.status.idle":"2022-10-19T03:06:20.223245Z","shell.execute_reply.started":"2022-10-19T03:06:19.759281Z","shell.execute_reply":"2022-10-19T03:06:20.219662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = output(location)\nimg","metadata":{"execution":{"iopub.status.busy":"2022-10-19T03:06:20.225327Z","iopub.execute_input":"2022-10-19T03:06:20.225874Z","iopub.status.idle":"2022-10-19T03:06:21.021476Z","shell.execute_reply.started":"2022-10-19T03:06:20.225835Z","shell.execute_reply":"2022-10-19T03:06:21.0207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('FruitModel.h5')","metadata":{"execution":{"iopub.status.busy":"2022-10-19T03:06:21.023454Z","iopub.execute_input":"2022-10-19T03:06:21.024025Z","iopub.status.idle":"2022-10-19T03:06:21.284379Z","shell.execute_reply.started":"2022-10-19T03:06:21.023985Z","shell.execute_reply":"2022-10-19T03:06:21.283602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6 Deploy the Model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}